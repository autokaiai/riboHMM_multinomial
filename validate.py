# Copyright 2025 Kai Alois WÃ¶llstein
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Validate a Pre-trained riboHMM Model.

This script provides a comprehensive validation suite for a pre-trained riboHMM
model. It takes a model, a test dataset, the model's configuration, and ground-
truth ORF annotations to generate a detailed, multi-page PDF report. The report
includes visualizations of the model's learned parameters, a quantitative
analysis of its performance in identifying start codons, and qualitative examples
of its decoding on specific transcripts.

Core Functionality:
1.  Parses command-line arguments for all required input files and the output report path.
2.  Loads the pre-trained model, test data, configuration, and ORF annotations.
3.  Generates a multi-page PDF report containing:
    - A summary page of the validation run parameters.
    - Custom-formatted heatmaps of both initial and learned transition matrices.
    - Per-state comparison plots of initial vs. learned emission probabilities.
    - A quantitative analysis of start codon prediction accuracy, including a
      boxplot/stripplot visualization and key performance metrics.
    - Detailed visualizations for 10 transcripts sampled at read count percentiles,
      featuring P-site aligned read counts and the decoded state path.
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from hmmlearn import hmm
from matplotlib.colors import ListedColormap
from matplotlib.backends.backend_pdf import PdfPages
from tqdm import tqdm

from utils import calculate_tpm, create_lagging_windows, normalize_emissions

# --- Helper Function ---
# The calculate_tpm, create_lagging_windows, and normalize_emissions functions have been moved to utils.py


# --- Logging Configuration ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s",
    stream=sys.stdout,
)


def parse_arguments() -> argparse.Namespace:
    """Parses command-line arguments for the validation script."""
    parser = argparse.ArgumentParser(
        description="Validate a pre-trained riboHMM model and generate a report."
    )
    parser.add_argument(
        "--input_npz",
        type=Path,
        required=True,
        help="Path to the .npz file containing the test set data.",
    )
    parser.add_argument(
        "--trained_model",
        type=Path,
        required=True,
        help="Path to the .joblib model file generated by train.py.",
    )
    parser.add_argument(
        "--model_config",
        type=Path,
        required=True,
        help="The same JSON config file used during training.",
    )
    parser.add_argument(
        "--orf_annotations",
        type=Path,
        required=True,
        help="Path to the CSV file with known ORF coordinates.",
    )
    parser.add_argument(
        "--output_report",
        type=Path,
        required=True,
        help="Path for the final PDF validation report.",
    )
    args = parser.parse_args()
    args.output_report.parent.mkdir(parents=True, exist_ok=True)
    return args


def load_data(
    npz_path: Path, model_path: Path, config_path: Path, orf_path: Path
) -> tuple[hmm.MultinomialHMM, Dict[str, np.ndarray], Dict[str, Any], pd.DataFrame]:
    """Loads all necessary data and models for validation."""
    logging.info("Loading all inputs for validation...")
    try:
        model = joblib.load(model_path)
        logging.info(f"  -> Loaded trained model from {model_path}")
    except Exception as e:
        logging.error(f"Failed to load trained model: {e}")
        sys.exit(1)

    # Load the base model config from the JSON file first
    try:
        with open(config_path, "r") as f:
            model_config = json.load(f)
        logging.info(f"  -> Loaded model configuration from {config_path}")
    except Exception as e:
        logging.error(f"Failed to load model config file: {e}")
        sys.exit(1)

    # Now, load the NPZ data
    try:
        data_loader = np.load(npz_path, allow_pickle=True)
        transcript_data = {
            key: data_loader[key]
            for key in data_loader.files
        }
        logging.info(f"  -> Loaded {len(transcript_data)} transcripts from {npz_path}")
    except Exception as e:
        logging.error(f"Failed to load and parse NPZ file: {e}")
        sys.exit(1)

    try:
        orf_df = pd.read_csv(orf_path)
        orf_df.set_index("Transcript stable ID version", inplace=True)
        logging.info(f"  -> Loaded {len(orf_df)} ORF annotations from {orf_path}")
    except Exception as e:
        logging.error(f"Failed to load ORF annotations CSV: {e}")
        sys.exit(1)

    return model, transcript_data, model_config, orf_df


# The create_lagging_windows and normalize_emissions functions have been moved to utils.py


def main():
    """Main function to orchestrate the validation and report generation."""
    args = parse_arguments()
    model, transcript_data, model_config, orf_df = load_data(
        args.input_npz, args.trained_model, args.model_config, args.orf_annotations
    )

    state_map = model_config["state_map"]
    state_labels = list(state_map.keys())
    n_components = len(state_labels)
    window_size = model_config["WINDOW_SIZE"]
    lag_offset = model_config["LAG_OFFSET"]

    # Prepare initial parameters for comparison
    initial_transmat = np.array(model_config["initial_transition_matrix"])
    emission_blueprints = model_config["emission_blueprints"]
    unnormalized_emissions = np.array(
        [emission_blueprints[label] for label in state_labels]
    )
    initial_emissions = normalize_emissions(unnormalized_emissions)

    logging.info(f"Generating validation report at {args.output_report}...")
    with PdfPages(args.output_report) as pdf:
        # Page 1: Run Summary
        fig, ax = plt.subplots(figsize=(11.69, 8.27))
        ax.axis("off")
        summary_text = (
            f"riboHMM Validation Report\n\n"
            f"Run Timestamp: {datetime.now().isoformat()}\n"
            f"Test Data File: {args.input_npz.resolve()}\n"
            f"Trained Model File: {args.trained_model.resolve()}\n"
            f"Model Config File: {args.model_config.resolve()}\n"
            f"ORF Annotations File: {args.orf_annotations.resolve()}\n"
            f"Number of Transcripts in Test Set: {len(transcript_data)}\n"
        )
        ax.text(0.05, 0.95, summary_text, ha="left", va="top", fontsize=10, wrap=True)
        ax.set_title("Run Summary", fontsize=16, pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Page 2: Initial Transition Matrix (Custom Colors)
        fig, ax = plt.subplots(figsize=(12, 10))
        cmap_viridis = plt.cm.viridis(np.linspace(0, 1, 256))
        custom_cmap = ListedColormap(cmap_viridis)
        custom_cmap.set_under(color='lightgrey') # For 1.0
        custom_cmap.set_bad(color='grey') # For 0.0
        
        masked_transmat = np.ma.masked_where(initial_transmat == 0, initial_transmat)
        im = ax.imshow(masked_transmat, cmap=custom_cmap, vmin=1e-9, vmax=0.999)

        ax.set_title("Initial Transition Matrix")
        ax.set_xticks(np.arange(n_components))
        ax.set_yticks(np.arange(n_components))
        ax.set_xticklabels(state_labels, rotation=90)
        ax.set_yticklabels(state_labels)
        for i in range(n_components):
            for j in range(n_components):
                val = initial_transmat[i, j]
                if val > 0:
                    ax.text(j, i, f"{val:.3f}", ha="center", va="center", color="white" if val < 0.5 else "black")
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Page 3: Learned Transition Matrix (with Custom Colors)
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Apply the same custom colormap and masking logic
        masked_learned_transmat = np.ma.masked_where(model.transmat_ == 0, model.transmat_)
        im = ax.imshow(masked_learned_transmat, cmap=custom_cmap, vmin=1e-9, vmax=0.999)

        ax.set_title("Learned Transition Matrix")
        ax.set_xticks(np.arange(n_components))
        ax.set_yticks(np.arange(n_components))
        ax.set_xticklabels(state_labels, rotation=90)
        ax.set_yticklabels(state_labels)
        
        # Add text annotations
        for i in range(n_components):
            for j in range(n_components):
                val = model.transmat_[i, j]
                if val > 0:
                    ax.text(j, i, f"{val:.3f}", ha="center", va="center", color="white" if val < 0.5 else "black")
        
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Subsequent Pages: Emission Probabilities (One per state)
        x_labels_rel = np.arange(window_size) - lag_offset
        for i, label in enumerate(state_labels):
            fig, ax = plt.subplots(figsize=(12, 8))
            ax.plot(x_labels_rel, initial_emissions[i], 'x--', alpha=0.7, label=f'Initial "{label}"')
            ax.plot(x_labels_rel, model.emissionprob_[i], 'o-', label=f'Learned "{label}"')
            ax.set_title(f'Emission Probabilities: State "{label}"')
            ax.set_xlabel("Relative Position to 5' End of Window")
            ax.set_ylabel("Probability")
            ax.grid(True, linestyle='--', alpha=0.6)
            ax.set_ylim(0, 1)  # Set fixed Y-axis from 0 to 1
            ax.set_xticks(x_labels_rel) # Ensure integer ticks
            ax.legend()
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

        # Next Page: Quantitative ORF Prediction Analysis
        logging.info("Performing start codon validation on the test set...")
        start_codon_distances = []
        init_state_index = state_map['INIT']
        for tid in tqdm(transcript_data.keys(), desc="Validating start codons"):
            if tid not in orf_df.index:
                continue
            # Using pseudocounts here to match the training process
            read_counts = transcript_data[tid] + model_config["PSEUDO_COUNT_TEST"]
            X = create_lagging_windows(read_counts, window_size, lag_offset).astype(np.int32)
            if X.shape[0] == 0: continue

            _, sequence = model.decode(X, algorithm="viterbi")
            predicted_init_indices = np.where(sequence == init_state_index)[0]

            if len(predicted_init_indices) > 0:
                predicted_start = predicted_init_indices[0]
                true_start = orf_df.loc[tid, 'cDNA coding start']
                # Note: 'cDNA coding start' is 1-based from MANE annotations.
                # This subtraction calculates the distance from the HMM-predicted
                # P-site to the annotated start codon. An ideal result would be
                # a tight distribution around the expected P-site offset (e.g., -13 to -15 nt),
                # confirming the INIT state correctly identifies the start region.
                start_codon_distances.append(predicted_start - true_start)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        if start_codon_distances:
            distances_arr = np.array(start_codon_distances)
            frame_accuracy = np.mean(distances_arr % 3 == 0) * 100
            accuracy_pm3 = np.mean(np.abs(distances_arr) <= 3) * 100

            # Get detailed percentiles for reporting and axis limits
            p5, p25, p50, p75, p95 = np.percentile(distances_arr, [5, 25, 50, 75, 95])
            
            # Add a violin plot for density
            sns.violinplot(y=distances_arr, ax=ax, inner=None, color='lightgray', alpha=0.5)
            sns.boxplot(y=distances_arr, ax=ax, showfliers=False, width=0.2,
                        whis=[5, 95]) # Set whiskers to 5th and 95th percentiles
            sns.stripplot(y=distances_arr, ax=ax, color=".25", size=2, alpha=0.3)
            
            metrics_text = (
                f"Frame Accuracy: {frame_accuracy:.2f}%\n"
                f"Accuracy within Â±3 nt: {accuracy_pm3:.2f}%\n\n"
                f"Percentiles:\n"
                f" - 5th: {p5:.1f} nt\n"
                f" - 25th: {p25:.1f} nt\n"
                f" - Median: {p50:.1f} nt\n"
                f" - 75th: {p75:.1f} nt\n"
                f" - 95th: {p95:.1f} nt"
            )
            ax.text(0.95, 0.95, metrics_text, transform=ax.transAxes, ha='right', va='top', bbox=dict(boxstyle='round', fc='wheat', alpha=0.5))

        ax.set_title('Distance: Predicted INIT State vs. True Start Codon')
        ax.set_ylabel('Distance (nt) [Predicted - True]')
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.axhline(0, color='red', linestyle='--')

        # Cut off the y-axis to zoom in on the distribution (whiskers)
        if start_codon_distances:
            # Find the whisker positions from the boxplot
            q1, q3 = np.percentile(distances_arr, [25, 75])
            iqr = q3 - q1
            lower_whisker = max(distances_arr.min(), q1 - 1.5 * iqr)
            upper_whisker = min(distances_arr.max(), q3 + 1.5 * iqr)
            y_lim_buffer = (upper_whisker - lower_whisker) * 0.1
            ax.set_ylim(lower_whisker - y_lim_buffer, upper_whisker + y_lim_buffer)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Final Pages: Percentile-based examples
        logging.info("Generating percentile-based example visualizations...")
        raw_reads = {tid: np.sum(counts) for tid, counts in transcript_data.items()}
        transcript_lengths = {tid: len(counts) for tid, counts in transcript_data.items()}
        tpm_values = calculate_tpm(raw_reads, transcript_lengths)
        
        sorted_tids_by_tpm = sorted(tpm_values.keys(), key=lambda tid: tpm_values[tid])
        
        # Failsafe: if we have fewer than 100 transcripts, use all available transcripts
        num_transcripts = len(sorted_tids_by_tpm)
        if num_transcripts < 100:
            logging.info(f"Only {num_transcripts} transcripts available. Generating examples for all transcripts.")
            # Use all available indices
            sorted_indices = list(range(num_transcripts))
        else:
            logging.info("Generating 100 percentile-based examples (1st to 100th percentile).")
            # Generate 100 percentiles (1st to 100th)
            percentiles = np.linspace(1, 100, 100)
            
            # Ensure we have unique indices to avoid plotting the same transcript twice
            indices_to_plot = set()
            for p in percentiles:
                # Correctly calculate the index for the p-th percentile in the sorted list
                if len(sorted_tids_by_tpm) > 1:
                    # Use np.arange to get the indices from 0 to N-1
                    idx = int(np.percentile(np.arange(len(sorted_tids_by_tpm)), p, method="nearest"))
                else:
                    idx = 0
                # Ensure index is within bounds
                if idx >= len(sorted_tids_by_tpm):
                    idx = len(sorted_tids_by_tpm) - 1
                indices_to_plot.add(idx)

            # Convert set to sorted list to maintain order
            sorted_indices = sorted(list(indices_to_plot))
        
        for idx in sorted_indices:
            example_tid = sorted_tids_by_tpm[idx]
            
            # pseudocounts
            read_counts = transcript_data[example_tid] + model_config["PSEUDO_COUNT_TEST"]
            X_example = create_lagging_windows(read_counts, window_size, lag_offset).astype(np.int32)
            if X_example.shape[0] == 0: continue
            
            _, state_sequence = model.decode(X_example, algorithm="viterbi")
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})
            
            # Calculate the title based on whether we're using all transcripts or percentiles
            if num_transcripts < 100:
                # When using all transcripts, show the rank order
                title_percentile = f"Transcript {idx + 1} of {num_transcripts} (by TPM)"
            else:
                # Calculate the actual percentile for the title
                p = (idx / (len(sorted_tids_by_tpm) - 1)) * 100 if len(sorted_tids_by_tpm) > 1 else 100
                title_percentile = "Max Expression" if p >= 99.9 else f"{p:.0f}th Percentile (by TPM)"
            fig.suptitle(
                f"Example: {title_percentile} Transcript ({example_tid})\n"
                f"Total Reads: {raw_reads.get(example_tid, 0):,} | "
                f"TPM: {tpm_values.get(example_tid, 0):.2f}",
                fontsize=16
            )

            # For visualization, the read counts are shifted to align the P-site
            # with the 5' end position that the model decodes. This provides a
            # more intuitive view of where the HMM places states relative to ribosome dynamics.
            # A shift of 13 nucleotides is a common convention for P-site alignment.
            p_site_shift = 13
            shifted_reads = np.pad(read_counts, (p_site_shift, 0), 'constant', constant_values=0)[:-p_site_shift]

            ax1.plot(shifted_reads, label=f"Read Counts (P-Site Shifted by {p_site_shift} nt)", color='gray')
            ax1.set_ylabel("Read Count")
            ax1.grid(True, linestyle='--', alpha=0.5)
            if example_tid in orf_df.index:
                orf_info = orf_df.loc[example_tid]
                ax1.axvspan(orf_info['cDNA coding start'], orf_info['cDNA coding end'], color='green', alpha=0.2, label='Annotated ORF')
            ax1.legend(loc='upper right')
            ax1.set_xlim(0, len(read_counts))

            state_colors = plt.cm.turbo(np.linspace(0, 1, n_components))
            for i, label in enumerate(state_labels):
                ax2.fill_between(np.arange(len(state_sequence)), 0, 1, where=(state_sequence == i), color=state_colors[i], label=label, step='post')
            ax2.legend(loc='center left', bbox_to_anchor=(1.02, 0.5))
            ax2.set_xlabel("Nucleotide Position")
            ax2.set_yticks([])
            
            plt.tight_layout(rect=[0, 0, 0.9, 0.96])
            pdf.savefig(fig)
            plt.close(fig)
    
    logging.info("â Validation report generated successfully!")

if __name__ == "__main__":
    main() 