"""
Validate a Pre-trained riboHMM Model.

This script provides a comprehensive validation suite for a pre-trained riboHMM
model. It takes a model, a test dataset, the model's configuration, and ground-
truth ORF annotations to generate a detailed, multi-page PDF report. The report
includes visualizations of the model's learned parameters, a quantitative
analysis of its performance in identifying start codons, and qualitative examples
of its decoding on specific transcripts.

Core Functionality:
1.  Parses command-line arguments for all required input files and the output report path.
2.  Loads the pre-trained model, test data, configuration, and ORF annotations.
3.  Generates a multi-page PDF report containing:
    - A summary page of the validation run parameters.
    - Custom-formatted heatmaps of both initial and learned transition matrices.
    - Per-state comparison plots of initial vs. learned emission probabilities.
    - A quantitative analysis of start codon prediction accuracy, including a
      boxplot/stripplot visualization and key performance metrics.
    - Detailed visualizations for 10 transcripts sampled at read count percentiles,
      featuring P-site aligned read counts and the decoded state path.
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from hmmlearn import hmm
from matplotlib.colors import ListedColormap
from matplotlib.backends.backend_pdf import PdfPages
from tqdm import tqdm

# --- Helper Function ---
def calculate_tpm(
    read_counts: Dict[str, int], transcript_lengths: Dict[str, int]
) -> Dict[str, float]:
    """Calculates Transcripts Per Million (TPM) for each transcript."""
    rpk_values = {}
    
    # Calculate RPK (Reads Per Kilobase) for each transcript
    for tid, count in read_counts.items():
        length_kb = transcript_lengths.get(tid, 0) / 1000.0
        if length_kb > 0:
            rpk_values[tid] = count / length_kb
        else:
            rpk_values[tid] = 0

    # Calculate the "per million" scaling factor
    total_rpk = sum(rpk_values.values())
    if total_rpk == 0:
        return {tid: 0.0 for tid in read_counts.keys()}
    
    scaling_factor = total_rpk / 1_000_000.0

    # Calculate TPM for each transcript
    tpm_values = {tid: rpk / scaling_factor for tid, rpk in rpk_values.items()}
    return tpm_values

# --- Logging Configuration ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s",
    stream=sys.stdout,
)


def parse_arguments() -> argparse.Namespace:
    """Parses command-line arguments for the validation script."""
    parser = argparse.ArgumentParser(
        description="Validate a pre-trained riboHMM model and generate a report."
    )
    parser.add_argument(
        "--input_npz",
        type=Path,
        required=True,
        help="Path to the .npz file containing the test set data.",
    )
    parser.add_argument(
        "--trained_model",
        type=Path,
        required=True,
        help="Path to the .joblib model file generated by train.py.",
    )
    parser.add_argument(
        "--model_config",
        type=Path,
        required=True,
        help="The same JSON config file used during training.",
    )
    parser.add_argument(
        "--orf_annotations",
        type=Path,
        required=True,
        help="Path to the CSV file with known ORF coordinates.",
    )
    parser.add_argument(
        "--output_report",
        type=Path,
        required=True,
        help="Path for the final PDF validation report.",
    )
    args = parser.parse_args()
    args.output_report.parent.mkdir(parents=True, exist_ok=True)
    return args


def load_data(
    npz_path: Path, model_path: Path, config_path: Path, orf_path: Path
) -> tuple[hmm.MultinomialHMM, Dict[str, np.ndarray], Dict[str, Any], pd.DataFrame]:
    """Loads all necessary data and models for validation."""
    logging.info("Loading all inputs for validation...")
    try:
        model = joblib.load(model_path)
        logging.info(f"  -> Loaded trained model from {model_path}")
    except Exception as e:
        logging.error(f"Failed to load trained model: {e}")
        sys.exit(1)

    # Load the base model config from the JSON file first
    try:
        with open(config_path, "r") as f:
            model_config = json.load(f)
        logging.info(f"  -> Loaded model configuration from {config_path}")
    except Exception as e:
        logging.error(f"Failed to load model config file: {e}")
        sys.exit(1)

    # Now, load the NPZ data
    try:
        data_loader = np.load(npz_path, allow_pickle=True)
        transcript_data = {
            key: data_loader[key]
            for key in data_loader.files
        }
        logging.info(f"  -> Loaded {len(transcript_data)} transcripts from {npz_path}")
    except Exception as e:
        logging.error(f"Failed to load and parse NPZ file: {e}")
        sys.exit(1)

    try:
        orf_df = pd.read_csv(orf_path)
        orf_df.set_index("Transcript stable ID version", inplace=True)
        logging.info(f"  -> Loaded {len(orf_df)} ORF annotations from {orf_path}")
    except Exception as e:
        logging.error(f"Failed to load ORF annotations CSV: {e}")
        sys.exit(1)

    return model, transcript_data, model_config, orf_df


def create_lagging_windows(
    read_counts: np.ndarray, window_size: int, lag_offset: int, constant_values: int = 1
) -> np.ndarray:
    """Creates lagging windows from a 1D array of read counts.

    This function generates a 2D array of sliding windows over the input read
    counts. The `lag_offset` parameter allows the windows to be shifted relative
    to the start of the sequence, which is useful for aligning features like
    the P-site of a ribosome.

    Args:
        read_counts: A 1D NumPy array of read counts for a single transcript.
        window_size: The size of each sliding window.
        lag_offset: The number of positions to shift the window start. This is
                    achieved by prepending `lag_offset` constant values.
        constant_values: The value to use for padding. Defaults to 1.

    Returns:
        A 2D NumPy array where each row is a window of size `window_size`.
    """
    padded_counts = np.pad(
        read_counts, (lag_offset, 0), "constant", constant_values=constant_values
    )
    num_windows = len(read_counts)
    if num_windows == 0:
        return np.empty((0, window_size), dtype=np.int32)
    shape = (num_windows, window_size)
    strides = (padded_counts.strides[0], padded_counts.strides[0])
    windows = np.lib.stride_tricks.as_strided(
        padded_counts, shape=shape, strides=strides
    )
    return windows


def normalize_emissions(unnormalized_emissions: np.ndarray) -> np.ndarray:
    """Normalizes emission probabilities to sum to 1."""
    row_sums = unnormalized_emissions.sum(axis=1, keepdims=True)
    row_sums[row_sums == 0] = 1
    return unnormalized_emissions / row_sums


def main():
    """Main function to orchestrate the validation and report generation."""
    args = parse_arguments()
    model, transcript_data, model_config, orf_df = load_data(
        args.input_npz, args.trained_model, args.model_config, args.orf_annotations
    )

    state_map = model_config["state_map"]
    state_labels = list(state_map.keys())
    n_components = len(state_labels)
    window_size = model_config["WINDOW_SIZE"]
    lag_offset = model_config["LAG_OFFSET"]

    # Prepare initial parameters for comparison
    initial_transmat = np.array(model_config["initial_transition_matrix"])
    emission_blueprints = model_config["emission_blueprints"]
    unnormalized_emissions = np.array(
        [emission_blueprints[label] for label in state_labels]
    )
    initial_emissions = normalize_emissions(unnormalized_emissions)

    logging.info(f"Generating validation report at {args.output_report}...")
    with PdfPages(args.output_report) as pdf:
        # Page 1: Run Summary
        fig, ax = plt.subplots(figsize=(11.69, 8.27))
        ax.axis("off")
        summary_text = (
            f"riboHMM Validation Report\n\n"
            f"Run Timestamp: {datetime.now().isoformat()}\n"
            f"Test Data File: {args.input_npz.resolve()}\n"
            f"Trained Model File: {args.trained_model.resolve()}\n"
            f"Model Config File: {args.model_config.resolve()}\n"
            f"ORF Annotations File: {args.orf_annotations.resolve()}\n"
            f"Number of Transcripts in Test Set: {len(transcript_data)}\n"
        )
        ax.text(0.05, 0.95, summary_text, ha="left", va="top", fontsize=10, wrap=True)
        ax.set_title("Run Summary", fontsize=16, pad=20)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Page 2: Initial Transition Matrix (Custom Colors)
        fig, ax = plt.subplots(figsize=(12, 10))
        cmap_viridis = plt.cm.viridis(np.linspace(0, 1, 256))
        custom_cmap = ListedColormap(cmap_viridis)
        custom_cmap.set_under(color='lightgrey') # For 1.0
        custom_cmap.set_bad(color='grey') # For 0.0
        
        masked_transmat = np.ma.masked_where(initial_transmat == 0, initial_transmat)
        im = ax.imshow(masked_transmat, cmap=custom_cmap, vmin=1e-9, vmax=0.999)

        ax.set_title("Initial Transition Matrix")
        ax.set_xticks(np.arange(n_components))
        ax.set_yticks(np.arange(n_components))
        ax.set_xticklabels(state_labels, rotation=90)
        ax.set_yticklabels(state_labels)
        for i in range(n_components):
            for j in range(n_components):
                val = initial_transmat[i, j]
                if val > 0:
                    ax.text(j, i, f"{val:.3f}", ha="center", va="center", color="white" if val < 0.5 else "black")
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Page 3: Learned Transition Matrix (with Custom Colors)
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Apply the same custom colormap and masking logic
        masked_learned_transmat = np.ma.masked_where(model.transmat_ == 0, model.transmat_)
        im = ax.imshow(masked_learned_transmat, cmap=custom_cmap, vmin=1e-9, vmax=0.999)

        ax.set_title("Learned Transition Matrix")
        ax.set_xticks(np.arange(n_components))
        ax.set_yticks(np.arange(n_components))
        ax.set_xticklabels(state_labels, rotation=90)
        ax.set_yticklabels(state_labels)
        
        # Add text annotations
        for i in range(n_components):
            for j in range(n_components):
                val = model.transmat_[i, j]
                if val > 0:
                    ax.text(j, i, f"{val:.3f}", ha="center", va="center", color="white" if val < 0.5 else "black")
        
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Subsequent Pages: Emission Probabilities (One per state)
        x_labels_rel = np.arange(window_size) - lag_offset
        for i, label in enumerate(state_labels):
            fig, ax = plt.subplots(figsize=(12, 8))
            ax.plot(x_labels_rel, initial_emissions[i], 'x--', alpha=0.7, label=f'Initial "{label}"')
            ax.plot(x_labels_rel, model.emissionprob_[i], 'o-', label=f'Learned "{label}"')
            ax.set_title(f'Emission Probabilities: State "{label}"')
            ax.set_xlabel("Relative Position to 5' End of Window")
            ax.set_ylabel("Probability")
            ax.grid(True, linestyle='--', alpha=0.6)
            ax.set_ylim(0, 1)  # Set fixed Y-axis from 0 to 1
            ax.set_xticks(x_labels_rel) # Ensure integer ticks
            ax.legend()
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

        # Next Page: Quantitative ORF Prediction Analysis
        logging.info("Performing start codon validation on the test set...")
        start_codon_distances = []
        init_state_index = state_map['INIT']
        for tid in tqdm(transcript_data.keys(), desc="Validating start codons"):
            if tid not in orf_df.index:
                continue
            # Using pseudocounts here to match the training process
            read_counts = transcript_data[tid] + model_config["PSEUDO_COUNT_TEST"]
            X = create_lagging_windows(read_counts, window_size, lag_offset).astype(np.int32)
            if X.shape[0] == 0: continue

            _, sequence = model.decode(X, algorithm="viterbi")
            predicted_init_indices = np.where(sequence == init_state_index)[0]

            if len(predicted_init_indices) > 0:
                predicted_start = predicted_init_indices[0]
                true_start = orf_df.loc[tid, 'cDNA coding start']
                # Note: 'cDNA coding start' is 1-based from MANE annotations.
                # This subtraction calculates the distance from the HMM-predicted
                # P-site to the annotated start codon. An ideal result would be
                # a tight distribution around the expected P-site offset (e.g., -13 to -15 nt),
                # confirming the INIT state correctly identifies the start region.
                start_codon_distances.append(predicted_start - true_start)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        if start_codon_distances:
            distances_arr = np.array(start_codon_distances)
            frame_accuracy = np.mean(distances_arr % 3 == 0) * 100
            accuracy_pm3 = np.mean(np.abs(distances_arr) <= 3) * 100

            # Get detailed percentiles for reporting and axis limits
            p5, p25, p50, p75, p95 = np.percentile(distances_arr, [5, 25, 50, 75, 95])
            
            # Add a violin plot for density
            sns.violinplot(y=distances_arr, ax=ax, inner=None, color='lightgray', alpha=0.5)
            sns.boxplot(y=distances_arr, ax=ax, showfliers=False, width=0.2,
                        whis=[5, 95]) # Set whiskers to 5th and 95th percentiles
            sns.stripplot(y=distances_arr, ax=ax, color=".25", size=2, alpha=0.3)
            
            metrics_text = (
                f"Frame Accuracy: {frame_accuracy:.2f}%\n"
                f"Accuracy within ±3 nt: {accuracy_pm3:.2f}%\n\n"
                f"Percentiles:\n"
                f" - 5th: {p5:.1f} nt\n"
                f" - 25th: {p25:.1f} nt\n"
                f" - Median: {p50:.1f} nt\n"
                f" - 75th: {p75:.1f} nt\n"
                f" - 95th: {p95:.1f} nt"
            )
            ax.text(0.95, 0.95, metrics_text, transform=ax.transAxes, ha='right', va='top', bbox=dict(boxstyle='round', fc='wheat', alpha=0.5))

        ax.set_title('Distance: Predicted INIT State vs. True Start Codon')
        ax.set_ylabel('Distance (nt) [Predicted - True]')
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.axhline(0, color='red', linestyle='--')

        # Cut off the y-axis to zoom in on the distribution (whiskers)
        if start_codon_distances:
            # Find the whisker positions from the boxplot
            q1, q3 = np.percentile(distances_arr, [25, 75])
            iqr = q3 - q1
            lower_whisker = max(distances_arr.min(), q1 - 1.5 * iqr)
            upper_whisker = min(distances_arr.max(), q3 + 1.5 * iqr)
            y_lim_buffer = (upper_whisker - lower_whisker) * 0.1
            ax.set_ylim(lower_whisker - y_lim_buffer, upper_whisker + y_lim_buffer)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Final Pages: Percentile-based examples
        logging.info("Generating percentile-based example visualizations...")
        raw_reads = {tid: np.sum(counts) for tid, counts in transcript_data.items()}
        transcript_lengths = {tid: len(counts) for tid, counts in transcript_data.items()}
        tpm_values = calculate_tpm(raw_reads, transcript_lengths)
        
        sorted_tids_by_tpm = sorted(tpm_values.keys(), key=lambda tid: tpm_values[tid])
        
        percentiles = np.linspace(10, 100, 10)
        
        # Ensure we have unique indices to avoid plotting the same transcript twice
        indices_to_plot = set()
        for p in percentiles:
            # Correctly calculate the index for the p-th percentile in the sorted list
            if len(sorted_tids_by_tpm) > 1:
                # Use np.arange to get the indices from 0 to N-1
                idx = int(np.percentile(np.arange(len(sorted_tids_by_tpm)), p, method="nearest"))
            else:
                idx = 0
            # Ensure index is within bounds
            if idx >= len(sorted_tids_by_tpm):
                idx = len(sorted_tids_by_tpm) - 1
            indices_to_plot.add(idx)

        # Convert set to sorted list to maintain order
        sorted_indices = sorted(list(indices_to_plot))
        
        for idx in sorted_indices:
            # Calculate the actual percentile for the title
            p = (idx / (len(sorted_tids_by_tpm) - 1)) * 100 if len(sorted_tids_by_tpm) > 1 else 100
            example_tid = sorted_tids_by_tpm[idx]
            
            # pseudocounts
            read_counts = transcript_data[example_tid] + model_config["PSEUDO_COUNT_TEST"]
            X_example = create_lagging_windows(read_counts, window_size, lag_offset).astype(np.int32)
            if X_example.shape[0] == 0: continue
            
            _, state_sequence = model.decode(X_example, algorithm="viterbi")
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})
            
            title_percentile = "Max Expression" if p >= 99.9 else f"{p:.0f}th Percentile (by TPM)"
            fig.suptitle(
                f"Example: {title_percentile} Transcript ({example_tid})\n"
                f"Total Reads: {raw_reads.get(example_tid, 0):,} | "
                f"TPM: {tpm_values.get(example_tid, 0):.2f}",
                fontsize=16
            )

            # For visualization, the read counts are shifted to align the P-site
            # with the 5' end position that the model decodes. This provides a
            # more intuitive view of where the HMM places states relative to ribosome dynamics.
            # A shift of 13 nucleotides is a common convention for P-site alignment.
            p_site_shift = 13
            shifted_reads = np.pad(read_counts, (p_site_shift, 0), 'constant', constant_values=0)[:-p_site_shift]

            ax1.plot(shifted_reads, label=f"Read Counts (P-Site Shifted by {p_site_shift} nt)", color='gray')
            ax1.set_ylabel("Read Count")
            ax1.grid(True, linestyle='--', alpha=0.5)
            if example_tid in orf_df.index:
                orf_info = orf_df.loc[example_tid]
                ax1.axvspan(orf_info['cDNA coding start'], orf_info['cDNA coding end'], color='green', alpha=0.2, label='Annotated ORF')
            ax1.legend(loc='upper right')
            ax1.set_xlim(0, len(read_counts))

            state_colors = plt.cm.turbo(np.linspace(0, 1, n_components))
            for i, label in enumerate(state_labels):
                ax2.fill_between(np.arange(len(state_sequence)), 0, 1, where=(state_sequence == i), color=state_colors[i], label=label, step='post')
            ax2.legend(loc='center left', bbox_to_anchor=(1.02, 0.5))
            ax2.set_xlabel("Nucleotide Position")
            ax2.set_yticks([])
            
            plt.tight_layout(rect=[0, 0, 0.9, 0.96])
            pdf.savefig(fig)
            plt.close(fig)
    
    logging.info("✅ Validation report generated successfully!")

if __name__ == "__main__":
    main() 